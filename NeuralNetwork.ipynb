{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data training size: 436\n",
      "Data testing size: 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bramm\\AppData\\Local\\Temp\\ipykernel_4480\\967502231.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']] = df[['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']].replace({'yes': 1, 'no': 0}).astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('dataset/Housing.csv')\n",
    "\n",
    "# Replace 'yes'/'no' with 1/0\n",
    "df[['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']] = df[['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']].replace({'yes': 1, 'no': 0}).astype(int)\n",
    "\n",
    "# Encoding 'furnishingstatus'\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "df['furnishingstatus'] = LabelEncoder().fit_transform(df['furnishingstatus'])\n",
    "\n",
    "# Split features (X) and target (y)\n",
    "X = df.drop(\"price\", axis=1)\n",
    "y = df[\"price\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "print(f'Data training size: {len(X_train)}')\n",
    "print(f'Data testing size: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.29896907 0.4        0.33333333 1.         1.         0.\n",
      "  0.         0.         1.         0.33333333 0.         0.        ]\n",
      " [0.3814433  0.4        0.33333333 0.         1.         0.\n",
      "  1.         0.         1.         1.         0.         0.5       ]\n",
      " [0.14886598 0.2        0.         0.         1.         0.\n",
      "  1.         0.         1.         0.66666667 0.         0.        ]\n",
      " [0.06597938 0.4        0.         0.33333333 1.         0.\n",
      "  1.         0.         0.         0.         1.         1.        ]\n",
      " [0.1443299  0.4        0.         0.33333333 1.         0.\n",
      "  0.         0.         0.         0.         0.         1.        ]]\n",
      "[0.55       0.43333333 0.20666667 0.16       0.12      ]\n"
     ]
    }
   ],
   "source": [
    "# Scaling the input features using MinMaxScaler\n",
    "scaler_X = MinMaxScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "# Scale the target (price) using MinMaxScaler\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Check the scaled data\n",
    "print(X_train_scaled[:5])\n",
    "print(y_train_scaled[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_loss_derivative(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / y_true.size\n",
    "\n",
    "def mae_loss(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def mae_loss_derivative(y_true, y_pred):\n",
    "    return np.where(y_pred > y_true, 1, -1) / y_true.size\n",
    "\n",
    "def initialize_weights(input_size, hidden_size, output_size):\n",
    "    np.random.seed(42)\n",
    "    W1 = np.random.randn(input_size, hidden_size) * 0.001\n",
    "    b1 = np.zeros((1, hidden_size))\n",
    "    W2 = np.random.randn(hidden_size, output_size) * 0.001\n",
    "    b2 = np.zeros((1, output_size))\n",
    "    return W1, b1, W2, b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(X, W1) + b1 \n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2 \n",
    "    A2 = Z2  \n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "def backward_pass(X, y, Z1, A1, Z2, A2, W1, W2, b1, b2, loss_derivative, lr):\n",
    "    m = y.shape[0]\n",
    "\n",
    "    y = y.reshape(-1, 1)\n",
    "    \n",
    "    dZ2 = loss_derivative(y, A2)\n",
    "\n",
    "    dW2 = np.dot(A1.T, dZ2) / m\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "\n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * relu_derivative(Z1)\n",
    "\n",
    "    dW1 = np.dot(X.T, dZ1) / m\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    \n",
    "    return W1, b1, W2, b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, X_test, y_test, hidden_size, epochs, lr, batch_size=None, loss_func=mse_loss, loss_derivative_func=mse_loss_derivative, method='batch'):\n",
    "    input_size = X_train.shape[1]\n",
    "    output_size = 1\n",
    "    W1, b1, W2, b2 = initialize_weights(input_size, hidden_size, output_size)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for epoch in range(epochs + 1):\n",
    "        if method == 'stochastic':\n",
    "            indices = np.random.permutation(X_train.shape[0])\n",
    "            for i in indices:\n",
    "                X_batch = X_train[i:i+1]\n",
    "                y_batch = y_train[i:i+1]\n",
    "                Z1, A1, Z2, A2 = forward_pass(X_batch, W1, b1, W2, b2)\n",
    "                W1, b1, W2, b2 = backward_pass(X_batch, y_batch, Z1, A1, Z2, A2, W1, W2, b1, b2, loss_derivative_func, lr)\n",
    "\n",
    "        elif method == 'minibatch' and batch_size is not None:\n",
    "            indices = np.random.permutation(X_train.shape[0])\n",
    "            for i in range(0, X_train.shape[0], batch_size):\n",
    "                batch_indices = indices[i:i+batch_size]\n",
    "                X_batch = X_train[batch_indices]\n",
    "                y_batch = y_train[batch_indices]\n",
    "                Z1, A1, Z2, A2 = forward_pass(X_batch, W1, b1, W2, b2)\n",
    "                W1, b1, W2, b2 = backward_pass(X_batch, y_batch, Z1, A1, Z2, A2, W1, W2, b1, b2, loss_derivative_func, lr)\n",
    "\n",
    "        else: \n",
    "            Z1, A1, Z2, A2 = forward_pass(X_train, W1, b1, W2, b2)\n",
    "            W1, b1, W2, b2 = backward_pass(X_train, y_train, Z1, A1, Z2, A2, W1, W2, b1, b2, loss_derivative_func, lr)\n",
    "\n",
    "        train_predictions = forward_pass(X_train, W1, b1, W2, b2)[-1]\n",
    "        train_loss = loss_func(y_train, train_predictions)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        test_predictions = forward_pass(X_test, W1, b1, W2, b2)[-1]\n",
    "        test_loss = loss_func(y_test, test_predictions)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == epochs:\n",
    "            print(f'Epoch {epoch}, Training Loss: {train_loss}, Test Loss: {test_loss}')\n",
    "\n",
    "    return W1, b1, W2, b2, train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X, y, W1, b1, W2, b2, loss_func):\n",
    "    _, _, _, predictions = forward_pass(X, W1, b1, W2, b2)\n",
    "    loss = loss_func(y, predictions)\n",
    "    return loss, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(X_train, y_train, X_test, y_test, hidden_size=5, epochs=500, lr=0.001, batch_size=64):\n",
    "    results = {}\n",
    "\n",
    "    loss_functions = {'MSE': (mse_loss, mse_loss_derivative), 'MAE': (mae_loss, mae_loss_derivative)}\n",
    "    methods = ['batch', 'stochastic', 'minibatch']\n",
    "\n",
    "    for loss_name, (loss_func, loss_derivative_func) in loss_functions.items():\n",
    "        for method in methods:\n",
    "            print(f\"\\nRunning experiment with {loss_name} and {method} gradient descent\")\n",
    "            \n",
    "            W1, b1, W2, b2, train_losses, test_losses = train(X_train, y_train, X_test, y_test, hidden_size, epochs, lr, batch_size, loss_func, loss_derivative_func, method)\n",
    "            \n",
    "            final_train_loss = train_losses[-1]\n",
    "            final_test_loss, _ = test(X_test, y_test, W1, b1, W2, b2, loss_func)\n",
    "            results[f'{loss_name}_{method}'] = (final_train_loss, final_test_loss)\n",
    "            \n",
    "            print(f\"Final Training Loss: {final_train_loss:.3f}\")\n",
    "            print(f\"Final Test Loss: {final_test_loss:.3f}\")\n",
    "\n",
    "    print(\"\\nSummary Results:\")\n",
    "    for key, (train_loss, test_loss) in results.items():\n",
    "        print(f\"{key.upper()} - Training Loss: {train_loss:.3f}, Test Loss: {test_loss:.3f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment with MSE and batch gradient descent\n",
      "Epoch 0, Training Loss: 0.1072471519423707, Test Loss: 0.14209167714597784\n",
      "Epoch 10, Training Loss: 0.10723987803778846, Test Loss: 0.14208366263771427\n",
      "Epoch 20, Training Loss: 0.10723260480068106, Test Loss: 0.14207564883085713\n",
      "Epoch 30, Training Loss: 0.10722533223090844, Test Loss: 0.14206763572529868\n",
      "Epoch 40, Training Loss: 0.10721806032838425, Test Loss: 0.14205962332121946\n",
      "Epoch 50, Training Loss: 0.10721078909290664, Test Loss: 0.14205161161847918\n",
      "Final Training Loss: 0.107\n",
      "Final Test Loss: 0.142\n",
      "\n",
      "Running experiment with MSE and stochastic gradient descent\n",
      "Epoch 0, Training Loss: 0.04186460782624392, Test Loss: 0.06732452735090676\n",
      "Epoch 10, Training Loss: 0.02796945357184964, Test Loss: 0.04658579592932605\n",
      "Epoch 20, Training Loss: 0.027968390380342798, Test Loss: 0.046729074637523495\n",
      "Epoch 30, Training Loss: 0.02796750775782698, Test Loss: 0.04665006875835159\n",
      "Epoch 40, Training Loss: 0.027972300469214527, Test Loss: 0.046780814391412184\n",
      "Epoch 50, Training Loss: 0.027984669629240982, Test Loss: 0.04653032162382412\n",
      "Final Training Loss: 0.028\n",
      "Final Test Loss: 0.047\n",
      "\n",
      "Running experiment with MSE and minibatch gradient descent\n",
      "Epoch 0, Training Loss: 0.10506305400972388, Test Loss: 0.13968365433520022\n",
      "Epoch 10, Training Loss: 0.08621320178928897, Test Loss: 0.11875142450559838\n",
      "Epoch 20, Training Loss: 0.07194713615011408, Test Loss: 0.10267194137290687\n",
      "Epoch 30, Training Loss: 0.0612040512528061, Test Loss: 0.09035763087193774\n",
      "Epoch 40, Training Loss: 0.053059884288168185, Test Loss: 0.08084298119720042\n",
      "Epoch 50, Training Loss: 0.04691073339145584, Test Loss: 0.07350291497613273\n",
      "Final Training Loss: 0.047\n",
      "Final Test Loss: 0.074\n",
      "\n",
      "Running experiment with MAE and batch gradient descent\n",
      "Epoch 0, Training Loss: 0.28156591425404737, Test Loss: 0.3102333354270016\n",
      "Epoch 10, Training Loss: 0.281543394863667, Test Loss: 0.31021102400161016\n",
      "Epoch 20, Training Loss: 0.2815208755549361, Test Loss: 0.31018871319636154\n",
      "Epoch 30, Training Loss: 0.28149835624572284, Test Loss: 0.3101664023920971\n",
      "Epoch 40, Training Loss: 0.2814758369363027, Test Loss: 0.3101440915877134\n",
      "Epoch 50, Training Loss: 0.2814533176269662, Test Loss: 0.31012178078336533\n",
      "Final Training Loss: 0.281\n",
      "Final Test Loss: 0.310\n",
      "\n",
      "Running experiment with MAE and stochastic gradient descent\n",
      "Epoch 0, Training Loss: 0.1255554090782293, Test Loss: 0.16992169651304495\n",
      "Epoch 10, Training Loss: 0.12482725775555677, Test Loss: 0.16740290264299465\n",
      "Epoch 20, Training Loss: 0.1248672793875001, Test Loss: 0.16745507297778894\n",
      "Epoch 30, Training Loss: 0.13405853407869017, Test Loss: 0.1729317127070808\n",
      "Epoch 40, Training Loss: 0.15225968065898063, Test Loss: 0.18505973874190693\n",
      "Epoch 50, Training Loss: 0.15893549518141353, Test Loss: 0.18993711230652383\n",
      "Final Training Loss: 0.159\n",
      "Final Test Loss: 0.190\n",
      "\n",
      "Running experiment with MAE and minibatch gradient descent\n",
      "Epoch 0, Training Loss: 0.27474874730086607, Test Loss: 0.3034598166991382\n",
      "Epoch 10, Training Loss: 0.21061887054781114, Test Loss: 0.2433628467387755\n",
      "Epoch 20, Training Loss: 0.16446874150665208, Test Loss: 0.20358235962047927\n",
      "Epoch 30, Training Loss: 0.14123234487373817, Test Loss: 0.18505775508732153\n",
      "Epoch 40, Training Loss: 0.1321243898916112, Test Loss: 0.17702644573149603\n",
      "Epoch 50, Training Loss: 0.12798602569389206, Test Loss: 0.17305513164974345\n",
      "Final Training Loss: 0.128\n",
      "Final Test Loss: 0.173\n",
      "\n",
      "Summary Results:\n",
      "MSE_BATCH - Training Loss: 0.107, Test Loss: 0.142\n",
      "MSE_STOCHASTIC - Training Loss: 0.028, Test Loss: 0.047\n",
      "MSE_MINIBATCH - Training Loss: 0.047, Test Loss: 0.074\n",
      "MAE_BATCH - Training Loss: 0.281, Test Loss: 0.310\n",
      "MAE_STOCHASTIC - Training Loss: 0.159, Test Loss: 0.190\n",
      "MAE_MINIBATCH - Training Loss: 0.128, Test Loss: 0.173\n"
     ]
    }
   ],
   "source": [
    "experiment_results = run_experiments(X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled, hidden_size=50, epochs=50, lr=0.001, batch_size=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
